{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (Á•ûÊ•ΩÂùÇË¶ö„ÄÖ)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David Tolnay (dtolnay)',\n",
       " 'Stephen Celis (stephencelis)',\n",
       " 'Henrik Rydg√•rd (hrydgard)',\n",
       " 'Stefan Prodan (stefanprodan)',\n",
       " 'Matthias Urhahn (d4rken)',\n",
       " 'Shreyas Patil (PatilShreyas)',\n",
       " 'Mr.doob (mrdoob)',\n",
       " 'Christian Muehlhaeuser (muesli)',\n",
       " 'Anuken (Mindustry)',\n",
       " 'MathewSachin (Captura)',\n",
       " 'laurent22 (joplin)',\n",
       " 'cketti (OkHttpWithContentUri)',\n",
       " 'aneagoie (ztm-python-cheat-sheet)',\n",
       " 'yajra (laravel-datatables)',\n",
       " 'munen (emacs.d)',\n",
       " 'cpp-httplib (Jared Palmer)',\n",
       " 'razzle (PySimpleGUI)',\n",
       " 'PySimpleGUI (Andrew Gallant)',\n",
       " 'ripgrep (John Arundel)',\n",
       " 'script (Sindre Sorhus)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "#See url = 'https://github.com/trending/developers' with the chrome dev tools and scroll over the name\n",
    "#Those are the tags I need. On h1 I have the name and the main repo and on 'p' I have the username. \n",
    "\n",
    "\n",
    "tags = ['h1', 'p']\n",
    "text = [element.text for element in soup.find_all(tags)][2:]\n",
    "#Check the results, the two first element are not needed (headers and presentations)\n",
    "text\n",
    "\n",
    "new = []\n",
    "\n",
    "# #Iteration to clean strings\n",
    "\n",
    "for item in text: \n",
    "    x = item.strip()\n",
    "    new.append(x)\n",
    "\n",
    "# #Check the result\n",
    "new\n",
    "\n",
    "# # #Function to create chunks of name+username+repo\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "# # #I have to pass 3 to chunks to create lists of 3 items (name, user and repo)\n",
    "new_text = chunks(new, 3) \n",
    "new_text\n",
    "\n",
    "\n",
    "# # #Convert results to a list\n",
    "newest_text = list(new_text)\n",
    "newest_text\n",
    "# #Check the results\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(len(newest_text)):\n",
    "        element = newest_text[i][0] + ' (' + newest_text[i][1] + ')'  \n",
    "        result.append(element)\n",
    "\n",
    "result[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = soup.find_all('h2',{'class':'f3 text-normal'});\n",
    "trending_devs = [dev.text.strip().replace(' ','').replace('\\n\\n', ' ') for dev in table];\n",
    "trending_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minimaxir/big-list-of-naughty-strings', 'rusty1s/pytorch_geometric', 'espnet/espnet', 'public-apis/public-apis', 'donnemartin/system-design-primer', 'ranjian0/building_tool', 'sherlock-project/sherlocküîé', 'OpenMined/PySyft', 'open-mmlab/mmdetection', 'xillwillx/skiptracer', 'allenai/allennlp', 'explosion/spaCyüí´', 'zylo117/Yet-Another-EfficientDet-Pytorch', 'renatoviolin/next_word_prediction', 'anandpawara/Real_Time_Image_Animation', 'pytorch/fairseq', 'lyhue1991/eat_tensorflow2_in_30_days', 'zhanghang1989/ResNeSt', 'Rapptz/discord.py', 'google-research/big_transfer', 'TachibanaYoshino/AnimeGAN', 'shengqiangzhang/examples-of-web-crawlers', 'hunglc007/tensorflow-yolov4-tflite', 'ianzhao05/textshot', 'bitcoinbook/bitcoinbook']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "articles = soup.find_all('article')\n",
    "repo = []\n",
    "articles\n",
    "for a in articles:\n",
    "    clean = a.text.strip().replace('\\n\\n','').split()\n",
    "    if clean[0] != 'Popular':\n",
    "        repo.append(clean[1] + clean[2] + clean[3])\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minimaxir / big-list-of-naughty-strings',\n",
       " 'rusty1s / pytorch_geometric',\n",
       " 'espnet / espnet',\n",
       " 'public-apis / public-apis',\n",
       " 'donnemartin / system-design-primer',\n",
       " 'ranjian0 / building_tool',\n",
       " 'sherlock-project / sherlock',\n",
       " 'OpenMined / PySyft',\n",
       " 'open-mmlab / mmdetection',\n",
       " 'xillwillx / skiptracer',\n",
       " 'allenai / allennlp',\n",
       " 'explosion / spaCy',\n",
       " 'zylo117 / Yet-Another-EfficientDet-Pytorch',\n",
       " 'renatoviolin / next_word_prediction',\n",
       " 'anandpawara / Real_Time_Image_Animation',\n",
       " 'pytorch / fairseq',\n",
       " 'lyhue1991 / eat_tensorflow2_in_30_days',\n",
       " 'zhanghang1989 / ResNeSt',\n",
       " 'Rapptz / discord.py',\n",
       " 'google-research / big_transfer',\n",
       " 'TachibanaYoshino / AnimeGAN',\n",
       " 'shengqiangzhang / examples-of-web-crawlers',\n",
       " 'hunglc007 / tensorflow-yolov4-tflite',\n",
       " 'ianzhao05 / textshot',\n",
       " 'bitcoinbook / bitcoinbook']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "\n",
    "#On the h1 tag I find the name of the repo\n",
    "tags = ['h1']\n",
    "text = [element.text for element in soup.find_all(tags)][1:]\n",
    "#First element does not interest us. Check the result\n",
    "#text\n",
    "\n",
    "#Cleaning the results (step 1)\n",
    "new = []\n",
    "\n",
    "for item in text: \n",
    "    x = item.strip()     \n",
    "    new.append(x)\n",
    "\n",
    "#Check the results\n",
    "new\n",
    "\n",
    "#Cleaning (step 2)\n",
    "newest_list = []\n",
    "\n",
    "for element in new:\n",
    "    item = element.replace('\\n', '')\n",
    "    item2 = item.replace('     ', '')\n",
    "    newest_list.append(item2)\n",
    "\n",
    "newest_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1', 'https:/static/images/footer/wikimedia-button.png', 'https:/static/images/footer/poweredby_mediawiki_88x31.png']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "images = soup.find_all(\"img\")\n",
    "result = []\n",
    "for i in images:\n",
    "    result.append('https:' + i['src'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#mw-head\n",
      "#p-search\n",
      "https://en.wiktionary.org/wiki/Python\n",
      "https://en.wiktionary.org/wiki/python\n",
      "#Snakes\n",
      "#Ancient_Greece\n",
      "#Media_and_entertainment\n",
      "#Computing\n",
      "#Engineering\n",
      "#Roller_coasters\n",
      "#Vehicles\n",
      "#Weaponry\n",
      "#People\n",
      "#Other_uses\n",
      "#See_also\n",
      "/w/index.php?title=Python&action=edit&section=1\n",
      "/wiki/Pythonidae\n",
      "/wiki/Python_(genus)\n",
      "/w/index.php?title=Python&action=edit&section=2\n",
      "/wiki/Python_(mythology)\n",
      "/wiki/Python_of_Aenus\n",
      "/wiki/Python_(painter)\n",
      "/wiki/Python_of_Byzantium\n",
      "/wiki/Python_of_Catana\n",
      "/w/index.php?title=Python&action=edit&section=3\n",
      "/wiki/Python_(film)\n",
      "/wiki/Pythons_2\n",
      "/wiki/Monty_Python\n",
      "/wiki/Python_(Monty)_Pictures\n",
      "/w/index.php?title=Python&action=edit&section=4\n",
      "/wiki/Python_(programming_language)\n",
      "/wiki/CPython\n",
      "/wiki/CMU_Common_Lisp\n",
      "/wiki/PERQ#PERQ_3\n",
      "/w/index.php?title=Python&action=edit&section=5\n",
      "/w/index.php?title=Python&action=edit&section=6\n",
      "/wiki/Python_(Busch_Gardens_Tampa_Bay)\n",
      "/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n",
      "/wiki/Python_(Efteling)\n",
      "/w/index.php?title=Python&action=edit&section=7\n",
      "/wiki/Python_(automobile_maker)\n",
      "/wiki/Python_(Ford_prototype)\n",
      "/w/index.php?title=Python&action=edit&section=8\n",
      "/wiki/Colt_Python\n",
      "/wiki/Python_(missile)\n",
      "/wiki/Python_(nuclear_primary)\n",
      "/w/index.php?title=Python&action=edit&section=9\n",
      "/wiki/Python_Anghelo\n",
      "/w/index.php?title=Python&action=edit&section=10\n",
      "/wiki/PYTHON\n",
      "/w/index.php?title=Python&action=edit&section=11\n",
      "/wiki/Cython\n",
      "/wiki/Pyton\n",
      "/wiki/File:Disambig_gray.svg\n",
      "/wiki/Help:Disambiguation\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Python&oldid=955934391\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Disambiguation_pages\n",
      "/wiki/Category:Disambiguation_pages_with_short_description\n",
      "/wiki/Category:All_article_disambiguation_pages\n",
      "/wiki/Category:All_disambiguation_pages\n",
      "/wiki/Category:Animal_common_name_disambiguation_pages\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Special:MyContributions\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Python\n",
      "/w/index.php?title=Special:UserLogin&returnto=Python\n",
      "/wiki/Python\n",
      "/wiki/Talk:Python\n",
      "/wiki/Python\n",
      "/w/index.php?title=Python&action=edit\n",
      "/w/index.php?title=Python&action=history\n",
      "/wiki/Main_Page\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Wikipedia:Featured_content\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "//shop.wikimedia.org\n",
      "/wiki/Help:Contents\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "/wiki/Special:WhatLinksHere/Python\n",
      "/wiki/Special:RecentChangesLinked/Python\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Python&oldid=955934391\n",
      "/w/index.php?title=Python&action=info\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452\n",
      "/w/index.php?title=Special:CiteThisPage&page=Python&id=955934391&wpFormIdentifier=titleform\n",
      "https://commons.wikimedia.org/wiki/Category:Python\n",
      "/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen\n",
      "/w/index.php?title=Python&printable=yes\n",
      "https://af.wikipedia.org/wiki/Python\n",
      "https://als.wikipedia.org/wiki/Python\n",
      "https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86\n",
      "https://az.wikipedia.org/wiki/Python\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)\n",
      "https://be.wikipedia.org/wiki/Python\n",
      "https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)\n",
      "https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)\n",
      "https://da.wikipedia.org/wiki/Python\n",
      "https://de.wikipedia.org/wiki/Python\n",
      "https://eo.wikipedia.org/wiki/Pitono_(apartigilo)\n",
      "https://eu.wikipedia.org/wiki/Python_(argipena)\n",
      "https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86\n",
      "https://fr.wikipedia.org/wiki/Python\n",
      "https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0\n",
      "https://hr.wikipedia.org/wiki/Python_(razdvojba)\n",
      "https://io.wikipedia.org/wiki/Pitono\n",
      "https://id.wikipedia.org/wiki/Python\n",
      "https://ia.wikipedia.org/wiki/Python_(disambiguation)\n",
      "https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)\n",
      "https://it.wikipedia.org/wiki/Python_(disambigua)\n",
      "https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F\n",
      "https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)\n",
      "https://kg.wikipedia.org/wiki/Mboma_(nyoka)\n",
      "https://la.wikipedia.org/wiki/Python_(discretiva)\n",
      "https://lb.wikipedia.org/wiki/Python\n",
      "https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)\n",
      "https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)\n",
      "https://nl.wikipedia.org/wiki/Python\n",
      "https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3\n",
      "https://no.wikipedia.org/wiki/Pyton\n",
      "https://pl.wikipedia.org/wiki/Pyton\n",
      "https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)\n",
      "https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)\n",
      "https://sk.wikipedia.org/wiki/Python\n",
      "https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)\n",
      "https://sh.wikipedia.org/wiki/Python\n",
      "https://fi.wikipedia.org/wiki/Python\n",
      "https://sv.wikipedia.org/wiki/Pyton\n",
      "https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Python\n",
      "https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD\n",
      "https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86\n",
      "https://vi.wikipedia.org/wiki/Python\n",
      "https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "//creativecommons.org/licenses/by-sa/3.0/\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "table = soup.find_all('a')\n",
    "for link in table:\n",
    "    if 'href' in link.attrs:\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles changed: 11\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "#When parsing, I see the only class different when a title has changes is usctitlechanged. Therefore I have to count those\n",
    "txt = requests.get(url).text;\n",
    "count = txt.count('class=\"usctitlechanged\"');\n",
    "print(f'Number of titles changed: {count}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "names = soup.find_all(\"h3\", attrs={\"class\":\"title\"})\n",
    "result = [n.text.strip() for n in names]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>11:09:30.4</td>\n",
       "      <td>32.21 N</td>\n",
       "      <td>116.06 W</td>\n",
       "      <td>BAJA CALIFORNIA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>11:06:38.0</td>\n",
       "      <td>1.98 S</td>\n",
       "      <td>119.35 E</td>\n",
       "      <td>SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:57:36.8</td>\n",
       "      <td>37.94 N</td>\n",
       "      <td>27.11 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:50:02.0</td>\n",
       "      <td>24.02 S</td>\n",
       "      <td>67.25 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:41:29.3</td>\n",
       "      <td>37.85 N</td>\n",
       "      <td>26.96 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:37:57.2</td>\n",
       "      <td>37.84 N</td>\n",
       "      <td>26.82 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:31:56.1</td>\n",
       "      <td>38.05 N</td>\n",
       "      <td>27.28 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:30:29.4</td>\n",
       "      <td>37.80 N</td>\n",
       "      <td>27.13 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>10:05:55.0</td>\n",
       "      <td>24.18 S</td>\n",
       "      <td>67.47 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:59:40.6</td>\n",
       "      <td>29.77 S</td>\n",
       "      <td>71.56 W</td>\n",
       "      <td>OFFSHORE COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:59:01.0</td>\n",
       "      <td>36.05 N</td>\n",
       "      <td>117.55 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:58:01.0</td>\n",
       "      <td>0.28 N</td>\n",
       "      <td>83.39 W</td>\n",
       "      <td>OFF COAST OF ECUADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:49:58.4</td>\n",
       "      <td>37.90 N</td>\n",
       "      <td>26.49 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:48:10.5</td>\n",
       "      <td>37.61 N</td>\n",
       "      <td>26.87 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:48:07.1</td>\n",
       "      <td>18.13 N</td>\n",
       "      <td>67.10 W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:44:07.3</td>\n",
       "      <td>32.90 N</td>\n",
       "      <td>115.69 W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:40:43.2</td>\n",
       "      <td>39.03 N</td>\n",
       "      <td>16.07 E</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:39:30.2</td>\n",
       "      <td>37.85 N</td>\n",
       "      <td>26.95 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:36:12.7</td>\n",
       "      <td>37.80 N</td>\n",
       "      <td>26.94 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>09:35:08.8</td>\n",
       "      <td>37.88 N</td>\n",
       "      <td>26.77 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Time Latitude Longitude                      Region\n",
       "0   2020-11-02  11:09:30.4  32.21 N  116.06 W     BAJA CALIFORNIA, MEXICO\n",
       "1   2020-11-02  11:06:38.0   1.98 S  119.35 E         SULAWESI, INDONESIA\n",
       "2   2020-11-02  10:57:36.8  37.94 N   27.11 E              WESTERN TURKEY\n",
       "3   2020-11-02  10:50:02.0  24.02 S   67.25 W            SALTA, ARGENTINA\n",
       "4   2020-11-02  10:41:29.3  37.85 N   26.96 E  DODECANESE ISLANDS, GREECE\n",
       "5   2020-11-02  10:37:57.2  37.84 N   26.82 E  DODECANESE ISLANDS, GREECE\n",
       "6   2020-11-02  10:31:56.1  38.05 N   27.28 E              WESTERN TURKEY\n",
       "7   2020-11-02  10:30:29.4  37.80 N   27.13 E              WESTERN TURKEY\n",
       "8   2020-11-02  10:05:55.0  24.18 S   67.47 W            SALTA, ARGENTINA\n",
       "9   2020-11-02  09:59:40.6  29.77 S   71.56 W    OFFSHORE COQUIMBO, CHILE\n",
       "10  2020-11-02  09:59:01.0  36.05 N  117.55 W          CENTRAL CALIFORNIA\n",
       "11  2020-11-02  09:58:01.0   0.28 N   83.39 W        OFF COAST OF ECUADOR\n",
       "12  2020-11-02  09:49:58.4  37.90 N   26.49 E  DODECANESE ISLANDS, GREECE\n",
       "13  2020-11-02  09:48:10.5  37.61 N   26.87 E  DODECANESE ISLANDS, GREECE\n",
       "14  2020-11-02  09:48:07.1  18.13 N   67.10 W                 PUERTO RICO\n",
       "15  2020-11-02  09:44:07.3  32.90 N  115.69 W         SOUTHERN CALIFORNIA\n",
       "16  2020-11-02  09:40:43.2  39.03 N   16.07 E              SOUTHERN ITALY\n",
       "17  2020-11-02  09:39:30.2  37.85 N   26.95 E  DODECANESE ISLANDS, GREECE\n",
       "18  2020-11-02  09:36:12.7  37.80 N   26.94 E  DODECANESE ISLANDS, GREECE\n",
       "19  2020-11-02  09:35:08.8  37.88 N   26.77 E  DODECANESE ISLANDS, GREECE"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "earthquakes = soup.find('tbody', {'id': 'tbody'}).find_all(\"tr\");\n",
    "\n",
    "nelem = 20;\n",
    "latest_earthquakes = [];\n",
    "    \n",
    "for earthquake in earthquakes[:nelem]:\n",
    "    # Date and time\n",
    "    date, time = earthquake.find('td', {'class': 'tabev6'}).find('a').text.split();\n",
    "    # Latitude and longitude\n",
    "    lat_deg, lon_deg = earthquake.find_all('td', {'class': 'tabev1'});\n",
    "    lat_dir, lon_dir, magnitude = earthquake.find_all('td', {'class': 'tabev2'});\n",
    "    lat_deg = f\"{lat_deg.text.strip()} {lat_dir.text.strip()}\";\n",
    "    lon_deg = f\"{lon_deg.text.strip()} {lon_dir.text.strip()}\";\n",
    "    # Region\n",
    "    region = earthquake.find('td', {'class': 'tb_region'}).text.strip();\n",
    "    # Create list of information and append\n",
    "    earthquake_summary = [date, time, lat_deg , lon_deg, region];\n",
    "    latest_earthquakes.append(earthquake_summary);\n",
    "    \n",
    "df = pd.DataFrame(latest_earthquakes, columns=['Date', 'Time', 'Latitude', 'Longitude', 'Region']);\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter no longer uses html code.  Rendered in Javascript\n",
    "\n",
    "\n",
    "\n",
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, input your username: @ironhackams\n",
      "Account name not found...\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "username = input('Please, input your username: ')\n",
    "html = requests.get(url + username).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "try:\n",
    "    tweet_box = soup.find('li', {'class':'ProfileNav-item ProfileNav-item--tweets is-active'});\n",
    "    tweets = tweet_box.find('a').find('span', {'class':'ProfileNav-value'});\n",
    "    print(\"{} has {} number of tweets.\".format(username, tweets.get('data-count')))\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter no longer uses html code.  Rendered in Javascript\n",
    "\n",
    "\n",
    "\n",
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/ironhack'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, input your username: ironhackams\n",
      "Account name not found...\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "username = input('Please, input your username: ')\n",
    "html = requests.get(url + username).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "try:\n",
    "    tweet_box = soup.find('li', {'class':'ProfileNav-item ProfileNav-item--followers'});\n",
    "    tweets = tweet_box.find('a').find('span', {'class':'ProfileNav-value'});\n",
    "    print(\"{} has {} followers.\".format(username, tweets.get('data-count')))\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "6¬†168¬†000+ articles\n",
      "Espa√±ol\n",
      "1¬†630¬†000+ art√≠culos\n",
      "Êó•Êú¨Ë™û\n",
      "1¬†231¬†000+ Ë®ò‰∫ã\n",
      "Deutsch\n",
      "2¬†486¬†000+ Artikel\n",
      "–†—É—Å—Å–∫–∏–π\n",
      "1¬†665¬†000+ —Å—Ç–∞—Ç–µ–π\n",
      "Fran√ßais\n",
      "2¬†254¬†000+ articles\n",
      "Italiano\n",
      "1¬†639¬†000+ voci\n",
      "‰∏≠Êñá\n",
      "1¬†150¬†000+ Ê¢ùÁõÆ\n",
      "Portugu√™s\n",
      "1¬†044¬†000+ artigos\n",
      "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\n",
      "1¬†068¬†000+ ŸÖŸÇÿßŸÑÿ©\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "languages = soup.find_all('a', {'class': 'link-box'})\n",
    "for language in languages:\n",
    "    print(language.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell us whether you accept cookies\n",
      "Data topics\n",
      "Support links\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "topics = soup.findAll('h2')\n",
    "for topic in topics:\n",
    "    print(topic.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sino-Tibetan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indo-European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indo-European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "0  Mandarin Chinese\n",
       "1      Sino-Tibetan\n",
       "2           Sinitic\n",
       "3           Spanish\n",
       "4     Indo-European\n",
       "5           Romance\n",
       "6           English\n",
       "7     Indo-European\n",
       "8          Germanic\n",
       "9             Hindi"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,\"lxml\")\n",
    "languages = soup.find('table', {'class': 'wikitable sortable'}).find_all('a', attrs = {'title' : True});\n",
    "lang = []\n",
    "for i in range(10):\n",
    "    lang.append(languages[i].text)\n",
    "lang_df = pd.DataFrame(lang)\n",
    "lang_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter no longer uses html code.  Rendered in Javascript\n",
    "\n",
    "\n",
    "\n",
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, input your username: ironhackams\n",
      "Input number of tweets to scrape: 3\n",
      "Account name not found or tweet list is empty...\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "username = input('Please, input your username: ')\n",
    "n_tweets = int(input('Input number of tweets to scrape: '))\n",
    "html = requests.get(url + username).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "all_tweets = soup.find_all('div', {'class':'tweet'})\n",
    "\n",
    "if all_tweets:\n",
    "    for tweet in all_tweets[0:n_tweets]:\n",
    "        name = tweet.find('span', {'class': 'FullNameGroup'}).find('strong')\n",
    "        username = tweet.find('span', {'class': 'username'})\n",
    "        time = tweet.find('small', {'class': 'time'})\n",
    "        content = tweet.find('p', {'class': 'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'})\n",
    "        statistics = tweet.find('div', {'class': 'ProfileTweet-actionCountList u-hiddenVisually'})\n",
    "        \n",
    "        print(f'\\n{name.text} {username.text} {time.text.strip()}')\n",
    "        print(content.text)\n",
    "        print(statistics.text.strip().replace('\\n', ' '))\n",
    "else:\n",
    "    print('Account name not found or tweet list is empty...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release</th>\n",
       "      <th>Director</th>\n",
       "      <th>Actors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>Tim Robbins &amp;  Morgan Freeman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando &amp;  Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino &amp;  Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale &amp;  Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda &amp;  Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>The Battle of Algiers</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>Brahim Hadjadj &amp;  Jean Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>The Terminator</td>\n",
       "      <td>1984</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>Arnold Schwarzenegger &amp;  Linda Hamilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Aladdin</td>\n",
       "      <td>1992</td>\n",
       "      <td>Ron Clements</td>\n",
       "      <td>Scott Weinger &amp;  Robin Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Winter Sleep</td>\n",
       "      <td>2014</td>\n",
       "      <td>Nuri Bilge Ceylan</td>\n",
       "      <td>Haluk Bilginer &amp;  Melisa S√∂zen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Tangerines</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>Lembit Ulfsak &amp;  Elmo N√ºganen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title Release              Director  \\\n",
       "0    The Shawshank Redemption    1994        Frank Darabont   \n",
       "1               The Godfather    1972  Francis Ford Coppola   \n",
       "2      The Godfather: Part II    1974  Francis Ford Coppola   \n",
       "3             The Dark Knight    2008     Christopher Nolan   \n",
       "4                12 Angry Men    1957          Sidney Lumet   \n",
       "..                        ...     ...                   ...   \n",
       "245     The Battle of Algiers    1966      Gillo Pontecorvo   \n",
       "246            The Terminator    1984         James Cameron   \n",
       "247                   Aladdin    1992          Ron Clements   \n",
       "248              Winter Sleep    2014     Nuri Bilge Ceylan   \n",
       "249                Tangerines    2013        Zaza Urushadze   \n",
       "\n",
       "                                       Actors  \n",
       "0               Tim Robbins &  Morgan Freeman  \n",
       "1                  Marlon Brando &  Al Pacino  \n",
       "2                 Al Pacino &  Robert De Niro  \n",
       "3              Christian Bale &  Heath Ledger  \n",
       "4                  Henry Fonda &  Lee J. Cobb  \n",
       "..                                        ...  \n",
       "245             Brahim Hadjadj &  Jean Martin  \n",
       "246   Arnold Schwarzenegger &  Linda Hamilton  \n",
       "247           Scott Weinger &  Robin Williams  \n",
       "248            Haluk Bilginer &  Melisa S√∂zen  \n",
       "249             Lembit Ulfsak &  Elmo N√ºganen  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "\n",
    "movies = soup.find_all('td', {'class':'titleColumn'})\n",
    "titles = [movie.find('a').text for movie in movies]\n",
    "years = [movie.find('span').text[1:-1] for movie in movies]\n",
    "directors = [movie.find('a').get('title').split(',')[0][:-7] for movie in movies]\n",
    "actors = [' & '.join(movie.find('a').get('title').split(',')[1:]) for movie in movies]\n",
    "\n",
    "movies_dict = {'Title': titles, 'Release': years, 'Director': directors, 'Actors': actors}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_dict)\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Up</td>\n",
       "      <td>2009</td>\n",
       "      <td>78-year-old Carl Fredricksen travels to Paradi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>1999</td>\n",
       "      <td>An insomniac office worker and a devil-may-car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Be or Not to Be</td>\n",
       "      <td>1942</td>\n",
       "      <td>During the Nazi occupation of Poland, an actin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Beauty</td>\n",
       "      <td>1999</td>\n",
       "      <td>A sexually frustrated suburban father has a mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lock, Stock and Two Smoking Barrels</td>\n",
       "      <td>1998</td>\n",
       "      <td>A botched card game in London triggers four fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aladdin</td>\n",
       "      <td>1992</td>\n",
       "      <td>A kindhearted street urchin and a power-hungry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>2009</td>\n",
       "      <td>Two friends are searching for their long lost ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Room</td>\n",
       "      <td>2015</td>\n",
       "      <td>Held captive for 7 years in an enclosed space,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stand by Me</td>\n",
       "      <td>1986</td>\n",
       "      <td>After the death of one of his friends, a write...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Downfall</td>\n",
       "      <td>2004</td>\n",
       "      <td>Traudl Junge, the final secretary for Adolf Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title Release  \\\n",
       "0                                   Up    2009   \n",
       "1                           Fight Club    1999   \n",
       "2                   To Be or Not to Be    1942   \n",
       "3                      American Beauty    1999   \n",
       "4  Lock, Stock and Two Smoking Barrels    1998   \n",
       "5                              Aladdin    1992   \n",
       "6                             3 Idiots    2009   \n",
       "7                                 Room    2015   \n",
       "8                          Stand by Me    1986   \n",
       "9                             Downfall    2004   \n",
       "\n",
       "                                             Summary  \n",
       "0  78-year-old Carl Fredricksen travels to Paradi...  \n",
       "1  An insomniac office worker and a devil-may-car...  \n",
       "2  During the Nazi occupation of Poland, an actin...  \n",
       "3  A sexually frustrated suburban father has a mi...  \n",
       "4  A botched card game in London triggers four fr...  \n",
       "5  A kindhearted street urchin and a power-hungry...  \n",
       "6  Two friends are searching for their long lost ...  \n",
       "7  Held captive for 7 years in an enclosed space,...  \n",
       "8  After the death of one of his friends, a write...  \n",
       "9  Traudl Junge, the final secretary for Adolf Hi...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "from random import shuffle;\n",
    "\n",
    "n_random = 10;\n",
    "\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "movies = soup.find_all('td', {'class':'titleColumn'})\n",
    "\n",
    "shuffle(movies)\n",
    "\n",
    "titles = [movie.find('a').text for movie in movies[0:n_random]]\n",
    "years = [movie.find('span').text[1:-1] for movie in movies[0:n_random]]\n",
    "links_to_movies = [movie.find('a').get('href') for movie in movies[0:n_random]]\n",
    "\n",
    "summary = []\n",
    "for link in links_to_movies:\n",
    "    html = requests.get('https://www.imdb.com' + link).content;\n",
    "    soup = BeautifulSoup(html, \"lxml\");\n",
    "    summary.append(soup.find('div', {'class':'summary_text'}).text.strip());\n",
    "\n",
    "movies_dict = {'Title': titles, 'Release': years, 'Summary': summary}\n",
    "\n",
    "movies_df = pd.DataFrame(movies_dict)\n",
    "movies_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: Pittsburgh\n",
      "\n",
      "Pittsburgh's temperature: 0.07¬∞C \n",
      "Wind speed: 5.7 m/s\n",
      "Description: Overcast clouds\n",
      "Weather: Clouds\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "city = input('Enter the city: ').lower();\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\n",
    "weather_json = requests.get(url).json()\n",
    "\n",
    "print(\"\\n{}'s temperature: {}¬∞C \".format(city.capitalize(), weather_json['main']['temp']))\n",
    "print(\"Wind speed: {} m/s\".format(weather_json['wind']['speed']))\n",
    "print(\"Description: {}\".format(weather_json['weather'][0]['description'].capitalize()))\n",
    "print(\"Weather: {}\".format(weather_json['weather'][0]['main'].capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>¬£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>¬£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>¬£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>¬£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>¬£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>¬£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>¬£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>¬£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>¬£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>¬£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>¬£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>¬£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>¬£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>¬£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>¬£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>¬£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>¬£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>¬£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>¬£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>¬£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title   Price     Stock\n",
       "0                      A Light in the ...  ¬£51.77  In stock\n",
       "1                      Tipping the Velvet  ¬£53.74  In stock\n",
       "2                              Soumission  ¬£50.10  In stock\n",
       "3                           Sharp Objects  ¬£47.82  In stock\n",
       "4            Sapiens: A Brief History ...  ¬£54.23  In stock\n",
       "5                         The Requiem Red  ¬£22.65  In stock\n",
       "6            The Dirty Little Secrets ...  ¬£33.34  In stock\n",
       "7                 The Coming Woman: A ...  ¬£17.93  In stock\n",
       "8                     The Boys in the ...  ¬£22.60  In stock\n",
       "9                         The Black Maria  ¬£52.15  In stock\n",
       "10  Starving Hearts (Triangular Trade ...  ¬£13.99  In stock\n",
       "11                  Shakespeare's Sonnets  ¬£20.66  In stock\n",
       "12                            Set Me Free  ¬£17.46  In stock\n",
       "13    Scott Pilgrim's Precious Little ...  ¬£52.29  In stock\n",
       "14                      Rip it Up and ...  ¬£35.02  In stock\n",
       "15                  Our Band Could Be ...  ¬£57.25  In stock\n",
       "16                                   Olio  ¬£23.88  In stock\n",
       "17        Mesaerion: The Best Science ...  ¬£37.59  In stock\n",
       "18           Libertarianism for Beginners  ¬£51.33  In stock\n",
       "19                It's Only the Himalayas  ¬£45.17  In stock"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "html = requests.get(url).content;\n",
    "soup = BeautifulSoup(html, \"lxml\");\n",
    "books = soup.find_all('article', {'class': 'product_pod'})\n",
    "\n",
    "titles = [book.find('h3').text for book in books];\n",
    "prices = [book.find('p', {'class': 'price_color'}).text for book in books];\n",
    "stock = [book.find('p', {'class': 'instock availability'}).text.strip() for book in books]\n",
    "\n",
    "books_dict = {'Title': titles, 'Price': prices, 'Stock': stock}\n",
    "\n",
    "books_df = pd.DataFrame(books_dict)\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
